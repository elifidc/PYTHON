"""Copy of PROGRAMMING PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NBiQBFIOlMA5nbOPIYwLeg4n5D8EaXT7

This journal serves as documentation of my project, capturing the thought process, challenges, and insights made.
# **CONNECTING TO THE API AND LOADING IN THE DATA**
"""

#Packages and Libraries Used Throughout
import numpy as np
import pandas as pd
import random
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
import squarify
#!pip install wordcloud
import wordcloud
import json
import seaborn as sns
from sklearn import preprocessing

import nltk #Text processing functions package
from nltk import word_tokenize
import nltk
nltk.download('wordnet')
nltk.download('punkt_tab')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
import re #Regular expressions, finds patterns in text
from wordcloud import WordCloud
import os

from kagglehub import kagglehub

# Downloading latest version of Kaggle data
path = kagglehub.dataset_download("adhok93/presidentialaddress")


#Verifying
print("Path to dataset files:", path)

# List all files in the downloaded directory
downloaded_files = os.listdir(path)
print("Files in the dataset directory:", downloaded_files)

file_path = os.path.join(path, 'inaug_speeches.csv')


#ChatGPT to trouble shoot encoding below
inaugural_df = pd.read_csv(file_path, encoding='ISO-8859-1')

#print(inaugural_df)

#Checking file structure type
#type(inaugural_df)
#inaugural_df.columns



#Dropping an irrelevant index column that got carried over when importing the CSV
inaugural_df = inaugural_df.drop(columns=['Unnamed: 0'])

inaugural_df

"""Confirming successful column drop

Adding a word_count column to the inaugural_df DataFrame because I anticipate this will be useful soon
"""

word_counts = []
for speech in inaugural_df['text']:
  words = speech.split()
  count = len(words)
  word_counts.append(count)
inaugural_df['word_count'] = word_counts

inaugural_df.head()

"""# **DATA IS LOADED, NOW WHAT?**


By this point, I have managed to connect to the API, and begin quereying data. I will now focus on familiarizing myself with the API, and preprocessing the speech text for further analyses. Below are some notes I took while trying to figure out how I want to analyze this text data.


Text analysis, aka text mining, or natural language processing

Steps:
**Text Preprocessing **-->
 Lower case everything, removing stop words such as "the", "and", etc. If possible, try "stemming"/"lemmatization", aka turning "running" into "run". Lemmatization is more sophisticated as it takes word context into account.

**Text Representation** -->
Text data must be converted into a numerical or structured form that can be processed by algorithms.
**Bag of Word**s -->
Counts word frequency, no grammar no order
**Term Frequency-Inverse Document Frequency**-->
Weighs the importance of a word in a document relative to its frequency across all documents. It helps highlight the most relevant words in each document.


First, I will preprocess one speech and then turn it into a for loop to preprocess all speeches.

I need to figure out how to:
Store the preprocessed speeches in an organized and uniform way.
Isolate the data I am interested in.


Then I will tackle:

**EXPLORATORY DATA ANALYSIS**
"""

#Isolating one speech into a new variable for testing/verifying/playing around with purposes throughout project

george_washington_speech = inaugural_df[inaugural_df['Name'] == 'George Washington']['text'].iloc[0]

george_washington_speech

"""## **FUNCTION TO PREPROCESS SPEECHES**"""

#PROCESS SPEECHES



def preprocess_speech(speech):
  #Converting speeches to lower case
  speech = speech.lower()

#Breaking up hyphenated words into individual components
  speech = re.sub(r'[-]', ' ', speech)

  #Removing everything that is not A-Z, numbers, whitespace, and other unwanted
  speech = re.sub(r'[^a-z0-9\s]', '', speech)
  speech = speech.replace("\\U0097", "â€”")

#Tokenization and Lemmtization
  speech_tokens = word_tokenize(speech)
  lemmatizer = WordNetLemmatizer()
  speech_tokens = [lemmatizer.lemmatize(token) for token in speech_tokens]


#Filtering stop words
  stop_words = set(stopwords.words('english'))
  filtered_tokens = [word for word in speech_tokens if word not in stop_words]
  return filtered_tokens

"""TESTING THE PREPROCESS_SPEECH() FUNCTION ON THE TEST VARIABLE"""

new_gw_speech = preprocess_speech(george_washington_speech)
#print(new_gw_speech)

"""# **PREPROCESSING ALL SPEECHES VIA FOR LOOP**


Below, I will use my preprocess_speech() function in a for loop to process all the presidential speeches and store them in a new data structure.
Processed text that has been tokenized and lemmatized will be stored in the processed_speeches DataFrame, while other analyses not related to tokenization and lemmatization will be stored in inaugural_df. *The processed_speeches DF will be used for word frequency analysis and other statistical analysis The inaugural_df DF will focus on any analyses that focus on the raw text itself and sentiment analysis with AI.*
"""

processed_speeches = []
for _, row in inaugural_df.iterrows():

   president = row['Name']
   speech_text = row['text']

  #Applying preprocess function to text fields assigned as speech_text
   processed_speech = preprocess_speech(speech_text)
  #Adding the clean_speech onto the clean_speeches list, with corresponding president name
   processed_speeches.append({
        'president': president,
        'processed_speeches': processed_speech
    })


processed_speeches = pd.DataFrame(processed_speeches)
processed_speeches.head(20)

"""At this point, the data has been loaded, and the individual speeches of presidents have been cleaned, tokenized, lemmatized, and are ready for data exploration. I have manually checked to ensure that rows were not jumbled and that indexes were not disrupted during data transfer. I have two dataframes containing processed and raw versions of the speeches



#***BEGINNING OF DATA EXPLORATION***
"""

#Creating a function to count the number of unique tokens using set()
def count_unique_words(processed_speeches):
    return len(set(processed_speeches))

#Creating variable "unique_word_count" to store the number of unique words per president per speech


unique_word_count = processed_speeches['processed_speeches'].apply(count_unique_words)

processed_speeches['number_of_unique_words'] = unique_word_count #Creating a new column to store calc.



#Verifying the creating of the new number of unique words column
processed_speeches.head()

# Summary statistics for unique word counts
#print(processed_speeches['number_of_unique_words'].describe())

print(processed_speeches['number_of_unique_words'].mean())
print(processed_speeches['number_of_unique_words'].std())
print(processed_speeches['number_of_unique_words'].min())
print(processed_speeches['number_of_unique_words'].max())

"""When looking at the summary statistics, we see that the average unique word count across inaugural speeches is right under 650.
The standard deviation of the unique word count is 280, indicating a decent amount of variability across speeches.
The minimum unique word count is 58, while the maximum is 1579.

"""

max_unique_word_speech = processed_speeches.sort_values(by='number_of_unique_words', ascending=False).iloc[0]
print(max_unique_word_speech)

minimum_unique_word_speech = processed_speeches.sort_values(by='number_of_unique_words', ascending=True).iloc[0]
print(minimum_unique_word_speech)

"""George Washington's second inaugural address had the least number of unique words. When you examine the original speech, even with duplicate words, it is still the shortest inaugural address by far.

On the other hand, William Henry Harrison's inaugural address had 1579 unique words. When you examine the original speech, it contains 8,425 words.
"""

inaugural_df['word_count'].plot(kind='hist', bins=15, title="Total Word Count Distribution")
plt.xlabel("Word Count")
plt.show()



processed_speeches['number_of_unique_words'].plot(kind="hist", bins=20, title ="Unique Word Count Distribution")
plt.xlabel("Unique Word Count")
plt.show()
#processed_speeches['number_of_unique_words'].plot(kind='hist', bins=15, title="Distribution of Unique Word Counts")
#plt.show()

all_words = ' '.join(inaugural_df['text']).split()
word_counts = Counter(all_words)
most_common_words = word_counts.most_common(10)

"""***Guiding Question: What are the most used words, stop words excluded, across all inaugural addresses?***

Currently, the inaugural addresses are in the form of individual lists. These need to be flattened into a single string for comprehensive analysis.

The first time I analyzed the top used words, a few over-lemmatized words made their way into the mix. The unwanted_words list filters those out.

"""

#WORDS TO FILTER OUT
#Some are encoding errors, some are what I am assuming are over-lemmatized (if that is a word)
unwanted_words = ['ha', 'u', 'wa', "americau0092s"]

#Creating new variable named all_words to store all individual words used in the inaugural addresses and splitting.
all_words = ' '.join([word for sublist in processed_speeches['processed_speeches'] for word in sublist])
filtered_words = [word for word in all_words.split() if word not in unwanted_words and len(word) > 1]

filtered_word_counts = Counter(filtered_words)
most_common_words = filtered_word_counts.most_common(10)


print(most_common_words)

"""VISUALIZATIONS FOR MOST COMMON WORDS

"""

processed_speeches.columns

most_common_words.columns = ['Word', 'Frequency']

words, counts = zip(*most_common_words)

# Create a bar chart
plt.figure(figsize=(10, 6))
plt.bar(words, counts, color='skyblue')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Most Common Words Across Inaugural Speeches')
plt.xticks(rotation=45)
plt.show()

#Wordcloud
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import matplotlib.colors as mcolors

wc_colors = ["#800000", "#ADD8E6", "#003366","#B22222"]  # Maroon, Light Blue, Neutral Gray, via GPT
cmap = mcolors.LinearSegmentedColormap.from_list("custom_colors", wc_colors)

wordcloud = WordCloud(width=800, height=400, background_color='white', colormap=cmap).generate_from_frequencies(filtered_word_counts)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""Now that I am a little more familiar with language analysis, I want to examine top words in inaugural addresses in different time periods.

"""

#Extracting the year (last 4 characters of the date column) to filter speeches by year/time period
inaugural_df['Year'] = inaugural_df['Date'].apply(lambda x: x[-4:])



#When merging the year column onto the processed_speeches DF, I kept getting duplicate columns in the trial and error process. Code below was used to fix

#processed_speeches = processed_speeches.merge(inaugural_df[['Year']], left_index=True, right_index=True, suffixes=('', '_inaugural'))
#processed_speeches = processed_speeches.drop(columns=['Year_y', 'Year_x', 'Year_inaugural'])


processed_speeches.head()

# Adding year column to processed_speeches from inaugural_df
processed_speeches['Year'] = inaugural_df['Year']

#print(inaugural_df.head())
print(processed_speeches.head())

"""*** FILTERING FOR & ANALYZING SPEECHES ACROSS DIFFERENT TIME PERIODS***"""

#COLD WAR ERA


cold_war_speeches = processed_speeches[(processed_speeches['Year'].astype(int) >= 1947) & (processed_speeches['Year'].astype(int) <= 1994)]['processed_speeches']

cold_war_all_words = [word for speech in cold_war_speeches for word in speech]

filtered_words_cold_war = [word for word in cold_war_all_words if word not in unwanted_words and len(word) > 1]


cold_war_word_count = Counter(filtered_words_cold_war)

most_common_cold_war = cold_war_word_count.most_common(12)

print(most_common_cold_war)


import matplotlib.pyplot as plt

# Extract words and their frequencies using zip, my new best friend
words_cw, frequencies_cw = zip(*most_common_cold_war)

plt.figure(figsize=(10, 6))
plt.bar(words_cw, frequencies_cw, color="maroon")
plt.title('Most Common Words in Cold War Era Inaugural Speeches')
plt.xlabel('Words')
plt.xticks(rotation=45) #Used GPT to fix
plt.ylabel('Frequency')

most_common_cold_war = pd.DataFrame(most_common_cold_war)
most_common_new_deal_words= pd.DataFrame(most_common_new_deal_words)
most_common_post_911= pd.DataFrame(most_common_post_911)



most_common_cold_war.columns = ['Word', 'Frequency']
most_common_new_deal_words.columns = ['Word', 'Frequency']
most_common_post_911.columns = ['Word', 'Frequency']

#VISUALIZING COLD WAR

#Used GPT to figure out how to color code the bars of the barchart
most_common_cold_war['Color'] = most_common_cold_war['Frequency'].apply(
    lambda x: 'maroon' if x >= np.percentile(most_common_cold_war['Frequency'], 50) else 'skyblue'
)

# Plot the data using the color mapping
plt.figure(figsize=(10, 6))
sns.barplot(data=most_common_cold_war, x='Word', y='Frequency', hue='Color', dodge=False, palette={'maroon': 'maroon', 'skyblue': 'skyblue'})
plt.legend([], [], frameon=False)
plt.title('Most Common Words Across Cold War Speeches')
plt.xlabel('Words')
plt.xticks(rotation=45)
plt.ylabel('Frequency')
plt.show()


=
most_common_cold_war['Color'] = most_common_cold_war['Frequency'].apply(
    lambda x: 'maroon' if x >= np.percentile(most_common_cold_war['Frequency'], 50) else 'skyblue'
)

# Plot the data using the color mapping
plt.figure(figsize=(10, 6))
sns.barplot(data=most_common_cold_war, x='Word', y='Frequency', hue='Color', dodge=False, palette={'maroon': 'maroon', 'skyblue': 'skyblue'})
plt.legend([], [], frameon=False)
plt.title('Most Common Words Across Cold War Speeches')
plt.xlabel('Words')
plt.xticks(rotation=45)
plt.ylabel('Frequency')
plt.show()

"""I originally grouped together WWI and WWII, and later examine them individually."""

#WWI AND WWII GROUPED TOGETHER

wwi_wwii_speeches = processed_speeches[((processed_speeches['Year'].astype(int) >= 1914) &
                                          (processed_speeches['Year'].astype(int) <= 1918)) |
                                         ((processed_speeches['Year'].astype(int) >= 1939) &
                                          (processed_speeches['Year'].astype(int) <= 1945))]['processed_speeches']


world_wars_all_words = [word for speech in wwi_wwii_speeches for word in speech]

world_wars_filtered = [word for word in world_wars_all_words if word not in unwanted_words and len(word) >1]

world_wars_word_count = Counter(world_wars_filtered)

most_common_world_wars = world_wars_word_count.most_common(17)

print(most_common_world_wars)

#WWI (1914-1918)
wwi_speeches = processed_speeches[(processed_speeches['Year'].astype(int) >= 1914) &
                                  (processed_speeches['Year'].astype(int) <= 1918)]['processed_speeches']


wwi_all_words = [word for speech in wwi_speeches for word in speech]

wwi_filtered = [word for word in wwi_all_words if word not in unwanted_words and len(word) > 1]

wwi_word_count = Counter(wwi_filtered)

most_common_wwi = wwi_word_count.most_common(17)

print(most_common_wwi)


words_wwi, frequencies_wwi = zip(*most_common_wwi)

#VISUALIZING WWI
plt.figure(figsize=(10, 6))
plt.bar(words_wwi, frequencies_wwi, color='maroon')


plt.title('Most Common Words in WWI Inaugural Speeches')
plt.xlabel('Frequency')
plt.xticks(rotation=45)
plt.ylabel('Words')
plt.show()

# WWII (1939-1945)
wwii_speeches = processed_speeches[(processed_speeches['Year'].astype(int) >= 1939) &
                                   (processed_speeches['Year'].astype(int) <= 1945)]['processed_speeches']


wwii_all_words = [word for speech in wwii_speeches for word in speech]

wwii_filtered = [word for word in wwii_all_words if word not in unwanted_words and len(word) > 1]
wwii_word_count = Counter(wwii_filtered)

most_common_wwii = wwii_word_count.most_common(17)

#print(most_common_wwii)

#VISUALIZING WWII

words_wwii, frequencies_wwii = zip(*most_common_wwii)

plt.figure(figsize=(10, 6))
plt.bar(words_wwii, frequencies_wwii, color='maroon')


plt.title('Most Common Words in WWII Inaugural Speeches')
plt.xlabel('Words')
plt.xticks(rotation=45)
plt.ylabel('Frequency')
plt.show()

#CIVIL WAR

civil_war_speeches = processed_speeches[(processed_speeches['Year'].astype(int) >= 1860) &
                                        (processed_speeches['Year'].astype(int) <= 1865)]['processed_speeches']


civil_war_all_words = [word for speech in civil_war_speeches for word in speech]
civil_war_filtered = [word for word in civil_war_all_words if word not in unwanted_words and len(word)>1]

civil_war_count = Counter(civil_war_filtered)

most_common_civil_war_words = civil_war_count.most_common(15)


print(most_common_civil_war_words)

#VISUALIZING CIVIL WAR


# Extract words and their frequencies using zip
words_civil, frequencies_civil = zip(*most_common_world_wars)

# Plotting the bar chart
plt.figure(figsize=(10, 6))
plt.bar(words_civil, frequencies_civil, color='Maroon')  # Horizontal bar chart for better legibility

# Adding titles and labels
plt.title('Most Common Words in Civil War Era Inaugural Speeches')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)

#NEW DEAL ERA
new_deal_era_speeches = processed_speeches[(processed_speeches['Year'].astype(int) >= 1933) &
                                            (processed_speeches['Year'].astype(int) <= 1939)]['processed_speeches']
new_deal_era_all_words = [word for speech in new_deal_era_speeches for word in speech]
new_deal_era_filtered = [word for word in new_deal_era_all_words if word not in unwanted_words and len(word)>1]

new_deal_era_count = Counter(new_deal_era_filtered)

most_common_new_deal_words = new_deal_era_count.most_common(15)

words_newdeal, frequencies_newdeal = zip(*most_common_new_deal_words)



#VISUALIZING NEW DEAL


plt.figure(figsize=(10,6))
plt.bar(words_newdeal, frequencies_newdeal, color = 'maroon')
plt.title('Most Common Words in New Deal Era Inaugural Speeches')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)

#TRIGRAMS


all_speeches = ' '.join([speech for speech in inaugural_df['text'].dropna()])

all_words = all_speeches.split()


trigrams = ngrams(all_words, 3)

# Count the frequency of each trigram
trigram_counts = Counter(trigrams)

most_common_trigrams = trigram_counts.most_common(15)

for trigram, count in most_common_trigrams:
    print(' '.join(trigram), count)




trigrams_labels = [' '.join(trigram) for trigram, _ in most_common_trigrams]
trigrams_frequencies = [count for _, count in most_common_trigrams]


#VISUALIZING TRIGRAMS
plt.figure(figsize=(12, 8))
plt.barh(trigrams_labels, trigrams_frequencies, color='maroon')
plt.xlabel('Frequency', fontsize=12)
plt.ylabel('Trigrams', fontsize=12)
plt.yticks(fontsize=14)
plt.title('Top 15 Most Common Trigrams in All Speeches', fontsize=14)
plt.gca().invert_yaxis()  # To display the most frequent trigram on top
plt.tight_layout()
plt.show()

#POST 9/11

post_911_speeches = processed_speeches[
    (processed_speeches['Year'].astype(int) >= 2001) &
    (processed_speeches['Year'].astype(int) <= 2005)
]['processed_speeches']

post_911_all_words = [word for speech in post_911_speeches for word in speech]

filtered_words_post_911 = [word for word in post_911_all_words if word not in unwanted_words and len(word) > 1]
from collections import Counter


post_911_word_count = Counter(filtered_words_post_911)


most_common_post_911 = post_911_word_count.most_common(20)


print(most_common_post_911)


import matplotlib.pyplot as plt


words_post_911 = [word for word, _ in most_common_post_911]
frequencies_post_911 = [freq for _, freq in most_common_post_911]

#VISUALIZING POST 9/11
plt.figure(figsize=(10, 6))
plt.bar(words_post_911, frequencies_post_911, color='maroon')
plt.title('Most Common Words in Inaugural Speeches Post-9/11', fontsize=14)
plt.xlabel('Words', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.xticks(rotation=45)
plt.show()

"""# ***Attempting to track the use of specific terms to see how their prevalence changes over time.***"""

# Define the terms to track
keywords = ['freedom', 'hope', 'enemy', 'rights', 'military', 'recession', 'recover', 'security', 'peace']


for keyword in keywords:
    # Create an empty list to store the count of the keywords
    keyword_counts = []

    # Loop through each year in the dataset
    for year in inaugural_df['Year'].dropna().unique():
        # Get all speeches for that year and combine them into one long string
        speeches_for_year = inaugural_df[inaugural_df['Year'] == year]['text'].str.lower()

        # Initialize a count
        count_keyword = 0

        # Count the occurrences of the keyword
        for speech in speeches_for_year:
            count_keyword += speech.count(keyword)

        # Store the count
        keyword_counts.append((year, count_keyword))

    keyword_df = pd.DataFrame(keyword_counts, columns=['Year', f'{keyword}_Count'])

    # Plot that one current keyword
    plt.figure(figsize=(10, 6))


    plt.plot(keyword_df['Year'], keyword_df[f'{keyword}_Count'], marker='o', label=keyword)
    plt.xlabel('Year')
    plt.ylabel(f'Count of "{keyword}"')
    plt.title(f'Use of the Word "{keyword}" in Inaugural Speeches Over Time')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

processed_speeches.columns

"""**BEGINNING OF AI INTEGRATION/EXPLORATION**"""

#!pip install openai==0.28
#Had to pin older version to use? Most current version kept giving error


#import openai

"""

You are a political scientist/historian knowledgable about American history. You are focusing on speech content and

"""

#MY API KEY
#openai.api_key = REDACTED REDACTED REDACTED
#API KEY HERE


#Sentiment Analysis Function
#Input:Text, Output: Sentiment


def analyze_sentiment(text_chunk):
    messages = [
        {"role": "system", "content": "You are a sentiment analysis model and you are also knowledgable in political science and American history"},
        {"role": "user", "content": f"Analyze the following presidential speech and provide the sentiment of the speech. Focus on the content, tone, and underlying themes of the speech: {text_chunk}"}
    ]
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=messages,
        max_tokens=150,
        temperature=0.2,
    )
    sentiment = response['choices'][0]['message']['content'].strip()
    return sentiment

"""Through trial and error, I found that the most speeches I can process at once through GPT is ~3. I am very quickly and frequently timing out due to token limit reached.

"""

#CREATING A FUNCTION TO RANDOMLY SAMPLE SPEECHES FROM GIVEN DATA FRAME, COMBINE, AND PERFORM ONE COMPREHENSIVE ANALYSIS


def combine_random_speeches(df, sample_size):
    sampled_df = df.sample(n=sample_size, random_state=42)  # Randomly sample speeches
    combined_text = " ".join(sampled_df['text'].tolist())  # Combine all sampled texts into one
    return combined_text



# Sampling 3 random speeches and combining their text into the "sampled_text"
sampled_text = combine_random_speeches(inaugural_df, 2)


# Analyze sentiment of the finalized combinedtext
final_sentiment = analyze_sentiment(sampled_text)


print(f"Sentiment: {final_sentiment}")

"""The sentiment expressed in this text is predominantly patriotic, humble, and optimistic. The speaker expresses a deep love and respect for their country, its history, and its principles. They acknowledge the challenges and responsibilities that come with their position, but they also express hope and confidence in the country's future. They emphasize the importance of unity, cooperation, and adherence to the country's founding principles. The speaker also advocates for peace, justice, and prosperity, both domestically and internationally.

Now that I can sample a few speeches at a time, I plan to compare the sentiments of inaugural speeches with those of other presidential speeches. To do this, I will retrieve additional data using the Kaggle API and perform sentiment analysis on the new speeches. Since this analysis focuses solely on sentiment, I will skip some of the preprocessing steps that were applied to the inaugural speeches but are not necessary for this task.

***STATE OF THE UNION SPEECHES***
"""

#Connecting to API again

import kagglehub

# Download the dataset
path_sotu = kagglehub.dataset_download("jyronw/us-state-of-the-union-addresses-1790-2019")

print("Path to dataset files:", path_sotu)

sotu_df = pd.read_csv(f"{path_sotu}/state_ofthe_union_texts.csv")

sotu_df.head()
print(sotu_df.dtypes)

sotu_df.rename(columns={'Text': 'text'}, inplace=True)

"""I now have another type of speech imported in stored in the sotu_df, which holds raw state of the union speeches

I was able to successfully sample from random SOTU speeches, however, I am unable to analyze their sentiments currently due to them being too long. I tried sampling from the bottom 50% of speeches by word count, and it was still too long. Upon look at summary statistics of SOTU speeches, it is clear they are significantly longer than inaugural speeches. Unfortunately, I now need to analyze sentiments one at a time, and I will log them manually. From there, I will either try and get one generalized sentiment, or generalize it myself. I am unable to complete this via for loop due to maxing out tokens per min. I tried to get around this with setting a delay and performing the sentiment analysis in small batches, but it is still ineffective, especially with the SOTU.
"""

word_counts = []
for speech in sotu_df['text']:
    words = speech.split()
    word_counts.append(len(words))


print(sotu_df.head())

##sorted_sotu_df = sotu_df.sort_values(by='word_count')
#short_speeches_df = sorted_sotu_df.head(int(len(sorted_sotu_df) * 0.3))

sotu_sample = combine_random_speeches(sotu_df, 1)
analyze_sentiment(sotu_sample)

"""Now, I will store these sentiments in a dictionary. I am unable to complete this with a loop due to tokens maxing out."""

sotu_sentiments = [
    {"trial_number": 1, "analysis": "The sentiment of this presidential speech is predominantly serious, determined, and patriotic. The speaker emphasizes the importance of unity, responsibility, and collective action in the face of grave challenges, which suggests a sentiment of resilience and determination. The tone is assertive and."},
    {"trial_number": 2, "analysis": "The sentiment of this presidential speech is predominantly positive and assertive, with a strong emphasis on unity, resilience, and commitment to democratic values. The tone is serious and urgent, reflecting the gravity of the global political situation at the time. The underlying themes of the speech include: **National Unity and Responsibility**: The president repeatedly calls for unity among the members of Congress and the American people, emphasizing the importance of collective action in the face of global threats. He also stresses the need for responsible debate and decision-making.**Defense and Security**: The speech outlines a comprehensive plan for strengthening the country's military forces and preparing for potential conflict. This includes increasing production of weapons and equipment, expanding the military's active strength, and providing economic"},
    {"trial_number": 3, "analysis": "The sentiment of this presidential speech is predominantly serious, determined, and hopeful. The speaker emphasizes the gravity of the situation, acknowledging the challenges faced by the nation and the world. The tone is solemn and resolute, but also optimistic, as the speaker expresses confidence in the ability of the Congress and the American people to rise to the occasion."},
    {"trial_number": 4, "analysis": "The sentiment of this presidential speech is predominantly positive and assertive, with a strong emphasis on unity, resilience, and determination. The tone is serious and urgent, reflecting the gravity of the global situation being addressed. The speech contains themes of national unity, responsibility, and defense preparedness"},
    {"trial_number": 5, "analysis": "The sentiment of this presidential speech is predominantly serious, determined, and patriotic. The tone is assertive and confident, with a strong emphasis on unity, responsibility, and the defense of freedom and justice. The speech is also filled with a sense of urgency and resolve, reflecting the grave international situation at the time."},
    {"trial_number": 6, "analysis": "The sentiment of this presidential speech is predominantly positive, assertive, and resolute. The speaker expresses a strong sense of national unity, determination, and confidence in the face of adversity. The tone is serious and urgent, reflecting the gravity of the situation being addressed. The themes of defense, unity, and democratic ideals are evident."},
    {"trial_number": 7, "analysis": "The sentiment of this presidential speech is predominantly serious, urgent, and resolute. The president is addressing a grave situation, emphasizing the importance of unity, responsibility, and action in the face of a significant threat. The tone is one of determination and resolve, with a clear call to action for Congress and the American people."},
    {"trial_number": 8, "analysis": "The sentiment of this presidential speech is predominantly serious, determined, and patriotic. The speaker emphasizes the importance of unity, responsibility, and the defense of freedom and justice, conveying a strong sense of national pride and commitment to democratic values. The tone is assertive and confident, but also acknowledges the gravity of the situation."},
    {"trial_number": 9, "analysis": "The sentiment of this presidential speech is predominantly positive and assertive, with a strong emphasis on unity, resilience, and commitment to democratic values. The speaker projects confidence in the nation's ability to face challenges and underscores the importance of collective action and responsibility. The tone is serious and urgent."},
    {"trial_number": 10, "analysis": "The overall sentiment of the speech is resolute, patriotic, and determined. The speaker emphasizes the importance of unity, responsibility, and the defense of freedom and justice. The tone is serious and urgent, reflecting the gravity of the situation at hand. The underlying themes include the defense of democracy, the threat of Soviet aggression, and the importance of collective security and international cooperation."}]

#API KEY
openai.api_key = REDACTED
#Creating a function to combined the sentiments into one final sentiment
def combine_sentiments(sentiment_list):
    # Combine the individual sentiment analyses into single text
    text_chunk = " ".join([f"Trial {entry['trial_number']}: {entry['analysis']}" for entry in sentiment_list])

    messages = [
        {"role": "system", "content": "You are a sentiment analysis model and you are also knowledgeable in political science and American history. You are being given a set of 10 sentiment analyses."},
        {"role": "user", "content": f"Generalize the set of sentiments given into one final sentiment. Focus on the content, tone, and underlying themes of the speech: {text_chunk}"}
    ]

    # My request to the model
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=messages,
        max_tokens=200,
        temperature=0.3,
    )


    final_sentiment = response['choices'][0]['message']['content'].strip()
    return final_sentiment

combine_sentiments(sotu_sentiments)

"""***OVERALL SENTIMENT FOR STATE OF THE UNION ADDRESSES***



The overall sentiment of these presidential speeches is predominantly serious, determined, and patriotic. The tone is assertive and urgent, reflecting the gravity of the situations being addressed. The speeches emphasize the importance of unity, responsibility, resilience, and collective action in the face of significant challenges. There is a strong commitment to democratic values, defense preparedness, and the defense of freedom and justice. The speeches project a sense of national pride, resolve, and optimism in the face of adversity.

Now, I will do the same thing for inaugural addresses
"""

inaugural_sentiments = [
    {"trial_number": 1, "analysis":"The sentiment of this presidential speech is largely positive with a strong emphasis on humility duty and patriotism. The speaker expresses a deep sense of responsibility and acknowledges the challenges ahead but remains optimistic about the future of the country. The tone of the speech is solemn and respectful reflecting the speaker's understanding of the gravity of the presidential office. The speaker frequently references the importance of unity the rule of law and the principles of justice and morality suggesting a commitment to democratic values and the well-being of the nation. The underlying themes of the speech include public service national unity and moral leadership. The speaker emphasizes the importance of serving the country selflessly maintaining national unity and leading with integrity."},
    {"trial_number": 2, "analysis":"The sentiment of this presidential speech is predominantly solemn humble and hopeful. The speaker acknowledges the weight of the responsibilities bestowed upon him and expresses his anxieties and self-perceived deficiencies. This humility is balanced by a strong sense of duty and commitment to the country and its citizens. The tone is respectful and reverential particularly towards the country its institutions and the citizens. The speaker frequently refers to the country's history its achievements and its potential evoking a sense of national pride and unity. He also expresses a deep respect for the democratic process and the Constitution. The underlying themes of the speech include patriotism duty humility unity and the importance of moral and ethical governance. The speaker emphasizes the importance of maintaining peace" },
    {"trial_number": 3, "analysis":"The sentiment of this presidential speech is predominantly humble patriotic and hopeful. The speaker expresses a deep sense of humility and acknowledges the weight of the responsibility that comes with the office. He also shows a profound love for his country and a strong commitment to its welfare and prosperity. The tone of the speech is solemn and respectful with the speaker expressing a deep sense of gratitude for the trust and confidence bestowed upon him by his fellow citizens. He also shows a strong sense of duty and a commitment to uphold the principles of justice peace and liberty. The underlying themes of the speech include patriotism duty humility and hope. The speaker emphasizes the importance of unity the preservation of liberty and the adherence to the principles of justice and peace."},
    {"trial_number": 4, "analysis":"The sentiment of this presidential speech is largely positive hopeful and humble. The speaker expresses a deep sense of duty gratitude and respect towards the country and its citizens. He acknowledges the challenges and responsibilities that come with his position but also expresses confidence in the principles and values that guide him. The speech is filled with themes of patriotism unity and faith. The speaker frequently references the importance of unity among the citizens and the different states and the necessity of adhering to the principles of the Constitution. He also emphasizes the importance of peace justice and neutrality in dealing with other nations. The speaker also expresses a deep sense of humility and acknowledges his own limitations. He frequently refers to his reliance on the wisdom and virtue of his fellow."},
    {"trial_number": 5, "analysis":"The sentiment of this presidential speech is predominantly humble solemn and hopeful. The speaker expresses a deep sense of responsibility and acknowledges the weight of the position he is about to assume. He is conscious of his own limitations and the challenges that lie ahead but he is also hopeful and confident in the strength and wisdom of the American people. The tone of the speech is respectful and reverent particularly when the speaker refers to the Almighty Being and the providential agency that has guided the nation's progress. He also expresses gratitude and devotion to his country and a deep respect for the democratic institutions and principles that define it. The underlying themes of the speech include patriotism duty humility and faith. The speaker emphasizes the"},
    {"trial_number": 6, "analysis":"The sentiment of this presidential speech is largely humble patriotic and optimistic. The speaker expresses a deep sense of responsibility and humility in accepting the role of president acknowledging his own perceived deficiencies and the magnitude of the task ahead. He also demonstrates a strong sense of patriotism and love for his country expressing his veneration and affection for the voice of his country and his ardent love for his nation. The speech is imbued with a sense of optimism and faith in the future of the country. The speaker expresses a belief in the principles of justice peace and liberty and the potential for the country to prosper under these principles. He also shows a strong faith in the wisdom and virtue of his fellow citizens and in the guidance of a higher"},
    {"trial_number": 7, "analysis":"This presidential speech is characterized by humility gratitude and solemnity. The sentiment is largely positive with the speaker expressing deep reverence and love for his country as well as a sense of duty and responsibility towards it. The speaker acknowledges the challenges and difficulties of the position he is about to assume but also expresses hope and confidence in the principles and values of his country and its citizens. The underlying themes include patriotism duty humility and faith. The speaker frequently references the importance of the country's republican institutions the principles of justice and peace and the need for unity and cooperation among its citizens. He also emphasizes the importance of maintaining a strong moral compass both in personal conduct and in the administration of government."},
    {"trial_number": 8, "analysis":"This presidential speech is solemn humble and hopeful. The speaker expresses a deep sense of responsibility and acknowledges the challenges ahead while conveying an unwavering commitment to the nation and its citizens. The speech is rich in themes of patriotism duty and public service, frequently referencing the nation's history republican institutions and guiding principles. It underscores the importance of unity peace justice and respect for individual rights and freedoms. The tone is respectful and deferential, with the speaker expressing gratitude for the trust placed in him and acknowledging his limitations while seeking guidance."},
    {"trial_number": 9, "analysis":"This presidential speech is humble solemn and hopeful. The speaker acknowledges the weight and responsibility of the office, expressing anxiety and personal inadequacy. However, they emphasize duty and commitment to the country, demonstrating deep love for their nation. The tone is serious and respectful, frequently invoking divine providence and expressing gratitude for the trust placed in them. The speech stresses unity, justice, peace, adherence to the Constitution, and a desire to uphold these values in their administration. Key themes include patriotism, humility, responsibility, and unity."},
    {"trial_number": 10, "analysis":"This presidential speech conveys humility, reverence, and a strong sense of duty. The sentiment is positive, with emphasis on unity, gratitude, and commitment to justice, peace, and liberty. The speaker acknowledges the challenges of the office, expressing anxiety about his qualifications but reaffirming his dedication to serve. References to a higher power indicate a belief in divine guidance. Patriotic sentiment runs through the speech, with the speaker emphasizing love for the country, unity, peace, and justice, and a belief in the nation's potential for greatness."}]

combine_sentiments(inaugural_sentiments)

"""*** OVERALL SENTIMENT FOR INAUGURAL ADDRESSES***
The overall sentiment of these presidential speeches is predominantly positive, humble, and hopeful. The speakers consistently express a deep sense of duty, responsibility, and patriotism, acknowledging the weight and challenges of the presidential office but maintaining an optimistic outlook for the future of the country. The tone across the speeches is solemn and respectful, with frequent references to the country's history, democratic institutions, and principles of justice, peace, and liberty. The speakers also demonstrate a profound reverence for their country and its citizens, emphasizing unity, moral leadership, and ethical governance. They acknowledge their own limitations and express gratitude for the trust placed in them by their fellow citizens. Underlying themes include patriotism, duty, humility, unity, and faith in divine guidance. The speeches convey a strong commitment to upholding the Constitution, maintaining national unity, and leading with integrity and respect for the values that define the nation. The speakers emphasize their dedication to serving the people with honesty, honor, and a deep commitment to the well-being and prosperity of the country

# ***Sentiment Analysis on Inaugural Speeches Filtered by Era***

In order to perform sentiment analysis on speeches filtered by era, I had to create new "era_name"_sentanal dataframes. When doing statistical analysis on word frequency per era, I was examining cleaned speeches with stop words removed, etc. I decided doing sentiment analysis on the raw text, with stopwords, would likely be more effective. This is why I am creating new dataframes based on the inaugural_df (contains raw speech), rather than use the previously made dataframes that were filtered for era. This way, I still have the raw data needed with the corresponding years and president name.
"""

inaugural_df['cleaned_text'] = inaugural_df['text'].apply(lambda x: ''.join(c for c in x if c.isprintable()))
#Line provided by ChatGPT to help me debug formatting issues

cold_war_sentanal = inaugural_df[(inaugural_df['Year'].astype(int) >= 1947) & (inaugural_df['Year'].astype(int) <= 1994)]['text']

wwi_wwii_sentanal = inaugural_df[((inaugural_df['Year'].astype(int) >= 1914) &
                                  (inaugural_df['Year'].astype(int) <= 1918)) |
                                 ((inaugural_df['Year'].astype(int) >= 1939) &
                                  (inaugural_df['Year'].astype(int) <= 1945))]['text']

new_deal_era_sentanal = inaugural_df[(inaugural_df['Year'].astype(int) >= 1933) &
                                     (inaugural_df['Year'].astype(int) <= 1939)]['text']

post_911_sentanal = inaugural_df[
    (inaugural_df['Year'].astype(int) >= 2001) &
    (inaugural_df['Year'].astype(int) <= 2005)]['text']

cold_war_sentanal = pd.DataFrame(cold_war_sentanal)
wwi_wwii_sentanal= pd.DataFrame(wwi_wwii_sentanal)
new_deal_era_sentanal= pd.DataFrame(new_deal_era_sentanal)
post_911_sentanal= pd.DataFrame(post_911_sentanal)

# Sampling 3 random speeches and combining their text into the "sampled_text"
#Sample size varies based on time period length

cw_sample = combine_random_speeches(cold_war_sentanal, 2)
ww_sample = combine_random_speeches(wwi_wwii_sentanal, 3)
new_deal_sample = combine_random_speeches(new_deal_era_sentanal,2)
post_911_sample = combine_random_speeches(post_911_sentanal, 2)


# Analyze sentiment of the finalized text and assign tot variable
cw_sentiment = analyze_sentiment(cw_sample)
ww_sentiment = analyze_sentiment(ww_sample)
new_deal_sentiment = analyze_sentiment(new_deal_sample)
post_911_sentiment = analyze_sentiment(post_911_sample)

print(cw_sentiment)

print(ww_sentiment)

print(new_deal_sentiment)

print(post_911_sentiment)
